#!/usr/bin/env python3
"""
Complete ingest pipeline - does everything in one go.
Runs: PDF cleaning â†’ Chunking â†’ Embedding â†’ Upsert to backends.
"""
from chunker import chunk_all_clean_files
from pdf_cleaner import clean_all_pdfs
from ingest_config import *
import gc  # For garbage collection
import sys
import time
from pathlib import Path

# Add current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))


def check_dependencies():
    """Check if all required dependencies are available"""
    print("ğŸ” Checking dependencies...")

    issues = []

    # Check directories
    if not Path(RAW_DIR).exists():
        issues.append(f"Raw directory missing: {RAW_DIR}")

    # Check for PDF files
    raw_path = Path(RAW_DIR)
    if raw_path.exists():
        pdf_files = list(raw_path.glob("*.pdf"))
        if not pdf_files:
            issues.append(f"No PDF files found in {RAW_DIR}")
        else:
            print(f"   ğŸ“š Found {len(pdf_files)} PDF files")

    # Check embedding model
    try:
        from sentence_transformers import SentenceTransformer
        print(f"   ğŸ¤– Embedding model: {EMBED_MODEL}")
    except ImportError:
        issues.append("sentence-transformers not installed")

    # Check backend availability
    if USE_QDRANT:
        try:
            from qdrant_client import QdrantClient
            print(f"   ğŸ—„ï¸  Qdrant client available")
        except ImportError:
            issues.append("qdrant-client not installed but USE_QDRANT=True")

    if USE_PGVECTOR:
        try:
            import psycopg2
            print(f"   ï¿½ PostgreSQL client available")
        except ImportError:
            issues.append("psycopg2 not installed but USE_PGVECTOR=True")

    if issues:
        print("âŒ Dependency issues found:")
        for issue in issues:
            print(f"   â€¢ {issue}")
        return False

    print("âœ… All dependencies satisfied")
    return True


def print_pipeline_config():
    """Show current pipeline configuration"""
    print("\nâš™ï¸  Pipeline Configuration")
    print("=" * 50)
    print(f"ğŸ“ Directories:")
    print(f"   Raw:   {RAW_DIR}")
    print(f"   Clean: {CLEAN_DIR}")

    print(f"\nğŸ¤– Embedding:")
    print(f"   Model:      {EMBED_MODEL}")
    print(f"   Dimensions: {QDRANT_VECTOR_SIZE}")
    print(f"   Distance:   {QDRANT_DISTANCE}")
    print(f"   Prefixes:   '{E5_QUERY_PREFIX}' / '{E5_PASSAGE_PREFIX}'")

    print(f"\nâœ‚ï¸  Chunking:")
    print(f"   Tokens:  {CHUNK_TOKENS}")
    print(f"   Overlap: {CHUNK_OVERLAP}")

    print(f"\nğŸ—„ï¸  Backends:")
    qdrant_status = "âœ… Enabled" if USE_QDRANT else "âŒ Disabled"
    pg_status = "âœ… Enabled" if USE_PGVECTOR else "âŒ Disabled"
    print(f"   Qdrant:     {qdrant_status} â†’ {QDRANT_COLLECTION}")
    print(f"   PostgreSQL: {pg_status} â†’ {PG_TABLE}")


def run_full_pipeline(skip_existing=True):
    """
    Run the complete ingest pipeline with enhanced PDF extraction.

    Args:
        skip_existing: Skip steps if output files already exist
    """
    print("ğŸš€ Starting Complete Ingest Pipeline (Enhanced & Memory Optimized)")
    print("=" * 60)

    start_time = time.time()

    try:
        # Step 1: Enhanced PDF cleaning (includes advanced extraction and chunking)
        print("\nï¿½ Step 1: Enhanced PDF Extraction & Cleaning")
        print("-" * 40)
        
        if skip_existing and Path(CLEAN_DIR).exists() and list(Path(CLEAN_DIR).glob("*.jsonl")):
            print("âœ… Clean files exist, skipping PDF extraction")
        else:
            print("ğŸ” Using enhanced PDF extraction with multiple libraries...")
            try:
                # Try enhanced PDF cleaner first
                from enhanced_pdf_cleaner import clean_all_pdfs_enhanced
                clean_all_pdfs_enhanced()
                print("âœ… Enhanced PDF extraction complete")
            except ImportError:
                print("âš ï¸  Enhanced PDF cleaner not available, using basic cleaner")
                clean_all_pdfs()
        
        # Force garbage collection
        gc.collect()
        
        # Step 2: Chunking (may already be done by enhanced extractor)
        print("\nâœ‚ï¸  Step 2: Text Chunking")
        print("-" * 40)
        
        chunk_files = list(Path(CLEAN_DIR).glob("*.chunks.jsonl"))
        clean_files = [f for f in Path(CLEAN_DIR).glob("*.jsonl") if not f.name.endswith(".chunks.jsonl")]
        
        if skip_existing and chunk_files:
            print(f"âœ… Found {len(chunk_files)} chunk files, skipping chunking")
        else:
            if clean_files:
                print(f"ğŸ“ Chunking {len(clean_files)} clean files...")
                chunk_all_clean_files()
                print("âœ… Chunking complete")
            else:
                print("âš ï¸  No clean files found for chunking")
        
        # Force garbage collection
        gc.collect()

        # Step 3: Embedding and database upsert (memory-safe)
        print("\nğŸ§  Step 3: Embedding & Database Upsert")
        print("-" * 40)
        
        # Check if we should use memory-safe embedding
        chunk_files = list(Path(CLEAN_DIR).glob("*.chunks.jsonl"))
        total_size = sum(f.stat().st_size for f in chunk_files) / (1024*1024)  # MB
        
        if total_size > 50:  # > 50MB of chunks
            print(f"ğŸ“¦ Large dataset detected ({total_size:.1f}MB), using memory-safe embedding...")
            try:
                from embed_safe import safe_embed_and_upsert
                success = safe_embed_and_upsert()
                if success:
                    print("âœ… Memory-safe embedding complete")
                else:
                    print("âŒ Memory-safe embedding failed")
            except ImportError:
                print("âš ï¸  Memory-safe embedder not available, using standard embedder")
                from embed_and_upsert import embed_and_upsert_all
                embed_and_upsert_all(clear_first=True)
        else:
            print(f"ğŸ“¦ Standard dataset ({total_size:.1f}MB), using standard embedding...")
            from embed_and_upsert import embed_and_upsert_all
            embed_and_upsert_all(clear_first=True)

        elapsed = time.time() - start_time
        print(f"\nğŸ‰ Pipeline completed in {elapsed:.1f} seconds!")
        
        # Show statistics
        show_pipeline_stats()

    except Exception as e:
        print(f"\nâŒ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        raise


def show_pipeline_stats():
    """Show statistics about the completed pipeline"""
    print("\nï¿½ Pipeline Statistics")
    print("-" * 30)

    try:
        clean_path = Path(CLEAN_DIR)

        # Count files at each stage
        pdf_files = list(Path(RAW_DIR).glob("*.pdf")
                         ) if Path(RAW_DIR).exists() else []
        clean_files = list(clean_path.glob("*.jsonl")
                           ) if clean_path.exists() else []
        clean_files = [
            f for f in clean_files if not f.name.endswith(".chunks.jsonl")]
        chunk_files = list(clean_path.glob("*.chunks.jsonl")
                           ) if clean_path.exists() else []

        print(f"ğŸ“š Source PDFs:    {len(pdf_files)}")
        print(f"ğŸ§¹ Clean files:    {len(clean_files)}")
        print(f"âœ‚ï¸  Chunk files:    {len(chunk_files)}")

        # Count total chunks
        total_chunks = 0
        for chunk_file in chunk_files:
            try:
                with open(chunk_file, "r") as f:
                    file_chunks = sum(1 for _ in f)
                    total_chunks += file_chunks
            except:
                pass

        print(f"ï¿½ Total chunks:   {total_chunks}")

        # Backend status
        if USE_QDRANT:
            print(f"ğŸ—„ï¸  Qdrant:        {QDRANT_COLLECTION}")
        if USE_PGVECTOR:
            print(f"ğŸ˜ PostgreSQL:    {PG_TABLE}")

    except Exception as e:
        print(f"âŒ Stats error: {e}")


def clean_pipeline():
    """Clean all pipeline outputs - start fresh"""
    print("ğŸ§¹ Cleaning Pipeline Outputs")
    print("-" * 30)

    import shutil

    # Clean directory
    clean_path = Path(CLEAN_DIR)
    if clean_path.exists():
        try:
            shutil.rmtree(clean_path)
            print(f"âœ… Removed {CLEAN_DIR}")
        except Exception as e:
            print(f"âŒ Could not remove {CLEAN_DIR}: {e}")

    print("ğŸ§¹ Clean complete - ready for fresh pipeline run")


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Complete RAG Ingest Pipeline")
    parser.add_argument("--config", action="store_true",
                        help="Show configuration and exit")
    parser.add_argument("--stats", action="store_true",
                        help="Show statistics and exit")
    parser.add_argument("--clean", action="store_true",
                        help="Clean all outputs and exit")
    parser.add_argument("--force", action="store_true",
                        help="Force re-run all steps")
    parser.add_argument("--check", action="store_true",
                        help="Check dependencies and exit")

    args = parser.parse_args()

    if args.config:
        print_pipeline_config()
        return

    if args.stats:
        show_pipeline_stats()
        return

    if args.clean:
        clean_pipeline()
        return

    if args.check:
        if check_dependencies():
            print("âœ… Ready to run pipeline")
            sys.exit(0)
        else:
            print("âŒ Dependencies not satisfied")
            sys.exit(1)

    # Show configuration
    print_pipeline_config()

    # Check dependencies
    if not check_dependencies():
        print("\nâŒ Cannot proceed - fix dependency issues first")
        sys.exit(1)

    # Run the pipeline
    skip_existing = not args.force
    success = run_full_pipeline(skip_existing=skip_existing)

    if success:
        print("\nğŸ¯ Next Steps:")
        print("   â€¢ Test: python test_clean_pipeline.py")
        print("   â€¢ Demo: python demo_clean_pipeline.py")
        print("   â€¢ Start API: python -m app.main")
        print("   â€¢ Query: curl 'http://localhost:8080/ask?q=Â¿CuÃ¡les son las nubes?'")
        sys.exit(0)
    else:
        print("\nâŒ Pipeline failed - check errors above")
        sys.exit(1)


if __name__ == "__main__":
    main()
